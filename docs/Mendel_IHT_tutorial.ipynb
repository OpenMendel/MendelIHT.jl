{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMendel Tutorial on Iterative Hard Thresholding\n",
    "\n",
    "### Last update: 2/11/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia version\n",
    "\n",
    "`MendelIHT.jl` currently supports Julia 1.0 and 1.1, but it currently an unregistered package. To install, press `]` to invoke the package manager mode and install these packages by typing:\n",
    "\n",
    "```\n",
    "add https://github.com/OpenMendel/SnpArrays.jl\n",
    "add https://github.com/OpenMendel/MendelSearch.jl\n",
    "add https://github.com/OpenMendel/MendelBase.jl\n",
    "add https://github.com/biona001/MendelIHT.jl\n",
    "```\n",
    "\n",
    "For this tutorial you will also need a few registered packages. Add them by typing:\n",
    "\n",
    "```\n",
    "add DataFrames, Distributions,BenchmarkTools, Random, LinearAlgebra\n",
    "```\n",
    "\n",
    "For reproducibility, the computer spec and Julia version is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.0.3\n",
      "Commit 099e826241 (2018-12-18 01:34 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i7-3740QM CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.0 (ORCJIT, ivybridge)\n",
      "Environment:\n",
      "  JULIA_NUM_THREADS = 8\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control files for beginning users, direct function calls for advanced users\n",
    "\n",
    "Numerous functions exist in IHT, which differ in input/output formats and optimization options. Users are encouraged to explore these functions as-is by directly calling them on imported data, demonstrated in the last few examples. For the less computer savvy, we also prepared the option of running analysis by constructing a 'control file' that specifies all analysis parameters. The later option is less flexible than the former to manipulate datasets, but less error prone.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Iterative Hard Thresholding\n",
    "\n",
    "Continuous model selection is advantageous in situations where the multivariate nature of the regressors plays a significant role together. As an alternative to traditional SNP-by-SNP association testing, iterative hard-thresholing (IHT) performs continuous model selection on a GWAS dataset $\\mathbf{X} \\in \\{0, 1, 2\\}^{n \\times p}$ and continuous phenotype vector $\\mathbf{y}$ by minimizing the residual sum of squares $f(\\beta) = \\frac{1}{2}||\\mathbf{y} - \\mathbf{X}\\beta||^2$ subject to the constraint that $\\beta$ is $k-$sparse. This method has the edge over LASSO (which also provides continuous model selection) because IHT does not shrink estimated effect sizes. Parallel computing is offered through `q-`fold cross validation, and in the near future, dense (genotype matrix)-(dense vector) multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriate Datasets and Example Inputs \n",
    "\n",
    "All genotype data **must** be stored in the [PLINK binary genotype format](https://www.cog-genomics.org/plink2/formats#bed), where the triplets `.bim`, `.bed` and `.fam` must all be present. Additional non-genetic covariates should be stored in a separate file (e.g. comma separated file). In the first 3 examples of this tutorial, we use \"gwas 1 data\" (github repo: [here](https://github.com/OpenMendel/MendelGWAS.jl/tree/master/docs)) to illustrate the basic usage and functionalities of MendelIHT. This dataset has 2200 people and a modest 10000 simulated SNPs, with 2 SNPs `rs1935681` and `rs2256412` (and an additional interaction term) contributing to the response . One can obtain this dataset from the first example input of [MendelGWAS.jl](https://openmendel.github.io/MendelGWAS.jl/), or via option 24a of the free application [Mendel version 16](http://software.genetics.ucla.edu/download?package=1). To examine the robustness and accuracy of IHT, examples 4 and 5 simulates data on the spot, and immediately calls IHT on the simulated data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "`MendelIHT` assumes there are no missing genotypes, since it uses linear algebra functions defined in [SnpArrays.jl](https://openmendel.github.io/SnpArrays.jl/latest/man/snparray/#linear-algebra-with-snparray). Therefore, you must first impute missing genotypes before you use MendelIHT. SnpArrays.jl offer a naive imputation strategy, but otherwise, our own software [option 23 of Mendel](http://software.genetics.ucla.edu/download?package=1) is a reasonable choice. Open Mendel will soon provide a separate package `MendelImpute.jl` containing new imputation strategies such as alternating least squares.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Regularization paths\n",
    "\n",
    "We usually have very little information on how many SNPs are affecting the phenotype. In a typical GWAS study, anywhere between 1 to thousands of SNPs could play a role. Thus ideally, we can test many different models to find the best one. MendelIHT provides 2 ways for one to perform this automatically: user specified regulartization paths, and $q-$fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics). Users should know that, in the first method, increasing the number of predictors will almost always decrease the error, but as a result introduce overfitting. Therefore, in most practical situations, it is highly recommended to combine this method with cross validation. In $q-$fold cross validation, samples are divided into $q$ disjoint subsets, and IHT fits a model on $q-1$ of those sets data, then computes the [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) tested on the $qth$ samples. Each $q$ subsets are served as the test set exactly once. This functionality of `MendelIHT.jl` natively supports parallel computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis keywords available to users \n",
    "\n",
    "| Keyword | Default Value | Allowed value | Description |\n",
    "| --- | --- | --- | --- |\n",
    "|`predictors` | 0 | Positive integer | Max number of non-zero entries of $\\beta$ |\n",
    "|`non_genetic_covariates` | \"\" | File name on disk | Delimited file containing the non-genetic covariates for each sample |\n",
    "|`run_cross_validation` | false | boolean | Whether the user wants to run cross-validation |\n",
    "|`model_sizes` | \"\" | Integers stored in string separated by ',' | Different model sizes users wish to run IHT |\n",
    "|`cv_fold` | 0 | Positive integer | Number of disjoint subsets the samples should be divided into |\n",
    "|`max_groups` (\\*) | 1 | Integer | Total number of groups |\n",
    "|`group_membership` (\\*) | \"\" | File name on disk | File indicating group membership |\n",
    "|`prior_weights` (\\*) | \"\" | maf | How to scale predictors based on different weights |\n",
    "|`glm` (\\*) | \"normal\" | normal, logistic, poisson | Running generalized linear models for cases when $y$ is normal, binary, or count data\n",
    "\n",
    "+ (\\*) Indicates experimental features. We currently have no theoretical guarantees on their performance, therefore illustrations of these functionalities are omitted from this tutorial. Users should tread carefully with these features. \n",
    "+ A list of OpenMendel keywords common to most analysis package can be found [here](https://openmendel.github.io/MendelBase.jl/#keywords-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Run IHT with Only Genotype Data\n",
    "\n",
    "### Step 1: Preparing Input files\n",
    "\n",
    "In Open Mendel, all analysis parameters are specified via the [Control file](https://openmendel.github.io/MendelBase.jl/#control-file). Genotype data must be inputted via the PLINK binary format. The most basic control file to run IHT looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas_1_data\n",
      "\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 2"
     ]
    }
   ],
   "source": [
    ";cat \"./tutorial_data/gwas_1_Control_basic.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run MendelIHT\n",
    "\n",
    "To run IHT with a control file, execute the following in the Julia REPL or in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading DataFrames support into Gadfly.jl\n",
      "└ @ Gadfly /Users/biona001/.julia/packages/Gadfly/09PWZ/src/mapping.jl:228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/dev/MendelIHT/docs/tutorial_data\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = ./tutorial_data/gwas_1_Control_basic.txt\n",
      "  pedigree_file = gwas_1_data.fam\n",
      "  plink_input_basename = gwas_1_data\n",
      "  predictors = 2\n",
      "  snpdata_file = gwas_1_data.bed\n",
      "  snpdefinition_file = gwas_1_data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Reading in data\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:42\n",
      "┌ Info: Running normal IHT for model size k = 2 and groups J = 1\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     1.142543077468872\n",
       "Final loss:             1183.6890803570777\n",
       "Iterations:             4\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   2\n",
       "IHT estimated 2 nonzero coefficients.\n",
       "2×3 DataFrames.DataFrame\n",
       "│ Row │ Group │ Predictor │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m     │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼───────┼───────────┼─────────────┤\n",
       "│ 1   │ 1     │ 3981      │ 0.147624    │\n",
       "│ 2   │ 1     │ 7023      │ 0.269147    │\n",
       "\n",
       "Intercept of model = 0.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MendelIHT\n",
    "IHT(\"./tutorial_data/gwas_1_Control_basic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd(\"../\") #change back to original directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3: Interpreting the results\n",
    "\n",
    "Here the estimated model is the 3981th and 7023th predictor, corresponding to rs1935681 and rs2256412 in the `gwas 1 data.bim` file, which are the correct SNPs. The intercept of the model is given at the bottom of the table. Here the compute time is the time associated with computing the optimal $\\beta$ only, so it does not include other necessary processes, such as importing the data. \n",
    "\n",
    "**Note:** the `affected_designator = 2` simply indicates that the pedigree is a Plink .fam file, which must always be the case for `MendelIHT` because this analysis option only accepts PLINK binary format as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Including Non-Genetic Covariates\n",
    "\n",
    "Non-genetic covariates must be stored in a comma demited file, with the same number of rows as the number of samples. The intercept term (i.e. grand mean) must also be included in the file. If the user does not specify a non-genetic covariate file, `MendelIHT` will by default include an intercept in the estimated model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Non-Genetic Covariate File\n",
    "\n",
    "In this example, we generated one non-genetic covariate from a $N(0, 1)$ distribution. After including the grand mean, we saved the file in `gwas 1 noncov.txt` where the entries are separated by a tab. The first few lines of this file looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t-0.088704513339476\n",
      "1\t-0.9575873240069772\n",
      "1\t-0.9713258274139007\n",
      "1\t-0.9847900613424241\n",
      "1\t-0.5954781589540936\n",
      "1\t0.2124813875751884\n",
      "1\t2.28150775802523\n",
      "1\t1.7643235366779797\n",
      "1\t-0.3933262467789896\n",
      "1\t-0.1348394065324508\n"
     ]
    }
   ],
   "source": [
    ";head -10 \"./tutorial_data/gwas_1_noncov.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Corresponding Control File \n",
    "\n",
    "We need to tell MendelIHT that the covariates are separated by tabs. This can be specified via the [MendelBase](https://openmendel.github.io/MendelBase.jl/) keyword `field_separator` in the control file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas_1_data\n",
      "non_genetic_covariates = gwas_1_noncov.txt\n",
      "field_separator = '\t'\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 2"
     ]
    }
   ],
   "source": [
    ";cat \"./tutorial_data/gwas_1_Control_nongen.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run IHT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/dev/MendelIHT/docs/tutorial_data\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = ./tutorial_data/gwas_1_Control_nongen.txt\n",
      "  field_separator = \t\n",
      "  non_genetic_covariates = gwas_1_noncov.txt\n",
      "  pedigree_file = gwas_1_data.fam\n",
      "  plink_input_basename = gwas_1_data\n",
      "  predictors = 2\n",
      "  snpdata_file = gwas_1_data.bed\n",
      "  snpdefinition_file = gwas_1_data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Reading in data\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:42\n",
      "┌ Info: Running normal IHT for model size k = 2 and groups J = 1\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     0.7715671062469482\n",
       "Final loss:             1183.6890803570777\n",
       "Iterations:             4\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   2\n",
       "IHT estimated 2 nonzero coefficients.\n",
       "2×3 DataFrames.DataFrame\n",
       "│ Row │ Group │ Predictor │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m     │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼───────┼───────────┼─────────────┤\n",
       "│ 1   │ 1     │ 3981      │ 0.147624    │\n",
       "│ 2   │ 1     │ 7023      │ 0.269147    │\n",
       "\n",
       "Intercept of model = 0.0\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MendelIHT\n",
    "IHT(\"./tutorial_data/gwas_1_Control_nongen.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd(\"../\") #change back to original directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Observe that the resulting error and model is exactly the same, because the covariates we added is white noise, and thus, was not selected by IHT to be a significant predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Cross Validation\n",
    "\n",
    "In this example, users can run IHT on any number of model in at most 8 parallel threads. Empirically, running on $n$ threads achieves roughly $n/2$ fold speedup. Note that running $q$ fold cross validation on $r$ different models entails running IHT $q \\times r$ times. \n",
    "\n",
    "### Step 0: IMPORTANT in order to take advantage of multithreads, discontinue this notebook and julia. Have a terminal open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: IMPORTANT Execute following line in the terminal BEFORE starting notebook (or Julia REPL) \n",
    "export JULIA_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start notebook in the same terminal window and verify that notebook is indeed running with 8 threads:  \n",
    "Note that if you computer's capacity is less than 8, it will default to the largest number it can run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Specify the model sizes\n",
    "\n",
    "The paths should be inside quotes and separated by comma, specified via the keyword `model_sizes`. Each entry must be an integer. In this example, we tried to run IHT for model sizes $k = 1, 2, ..., 10$ and 5 different folds. This is equivalent to running IHT 50 different times, and hence, ideal for parallel computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas_1_data\n",
      "#\n",
      "# Cross Validation parameters\n",
      "#\n",
      "run_cross_validation = true\n",
      "cv_folds = 5\n",
      "model_sizes = \"1,2,3,4,5,6,7,8,9,10\"\n"
     ]
    }
   ],
   "source": [
    ";cat \"./tutorial_data/gwas_1_Control_cv.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Cross Validation to find best model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/dev/MendelIHT/docs/tutorial_data\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = ./tutorial_data/gwas_1_Control_cv.txt\n",
      "  cv_folds = 5\n",
      "  model_sizes = 1,2,3,4,5,6,7,8,9,10\n",
      "  pedigree_file = gwas_1_data.fam\n",
      "  plink_input_basename = gwas_1_data\n",
      "  run_cross_validation = true\n",
      "  snpdata_file = gwas_1_data.bed\n",
      "  snpdefinition_file = gwas_1_data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Reading in data\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:42\n",
      "┌ Info: Running 5-fold cross validation on the following model sizes:\n",
      "│ 1,2,3,4,5,6,7,8,9,10.\n",
      "│ Ignoring keyword predictors.\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "k\tMSE\n",
      "1\t0.5482980579413379\n",
      "2\t0.540387690843733\n",
      "3\t0.5281201972176272\n",
      "4\t0.5335837402453649\n",
      "5\t0.5356074075522838\n",
      "6\t0.5377695731430505\n",
      "7\t0.5395404937328938\n",
      "8\t0.5414253752022411\n",
      "9\t0.5476004945198947\n",
      "10\t0.5496105157367333\n",
      "\n",
      "The lowest MSE is achieved at k = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MendelIHT\n",
    "IHT(\"./tutorial_data/gwas_1_Control_cv.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd(\"../\") #change back to original directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Re-run ordinary IHT on the best model size\n",
    "\n",
    "According to our cross validation result, the best model size that minimizes out-of-sample errors (i.e. MSE on the q-th subset of samples) is attained at $k = 3$. That is, cross validation likely detected that we need 3 SNPs (one being the interaction term captured via the intercept) to achieve the best model size. Using this information, one can re-run the IHT code to obtain the estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas_1_data\n",
      "\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 3"
     ]
    }
   ],
   "source": [
    ";cat \"./tutorial_data/gwas_1_Control_basic2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/dev/MendelIHT/docs/tutorial_data\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = ./tutorial_data/gwas_1_Control_basic2.txt\n",
      "  pedigree_file = gwas_1_data.fam\n",
      "  plink_input_basename = gwas_1_data\n",
      "  predictors = 3\n",
      "  snpdata_file = gwas_1_data.bed\n",
      "  snpdefinition_file = gwas_1_data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Reading in data\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:42\n",
      "┌ Info: Running normal IHT for model size k = 3 and groups J = 1\n",
      "└ @ MendelIHT /Users/biona001/.julia/dev/MendelIHT/src/IHT_wrapper.jl:113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     0.8421881198883057\n",
       "Final loss:             1161.9511204211908\n",
       "Iterations:             4\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   3\n",
       "IHT estimated 2 nonzero coefficients.\n",
       "2×3 DataFrames.DataFrame\n",
       "│ Row │ Group │ Predictor │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mInt64\u001b[39m     │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼───────┼───────────┼─────────────┤\n",
       "│ 1   │ 1     │ 3981      │ 0.147625    │\n",
       "│ 2   │ 1     │ 7023      │ 0.269147    │\n",
       "\n",
       "Intercept of model = 0.14057571565526017\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IHT(\"./tutorial_data/gwas_1_Control_basic2.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd(\"../\") #change back to original directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Using simulated results to examine IHT signal reconstruction\n",
    "\n",
    "In this example, we want to explicitly examine the ability for IHT to recover signals. Note the follow 2 examples directly call functions of `MendelIHT.jl` instead of using control files. This allows greater flexibility on end users because they can directly manipulate variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate data\n",
    "\n",
    "We first simulate a uniform random SNP matrix $X$ and a known model $\\beta$ as follows:\n",
    "$$\\beta \\sim N(0, 1),$$\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "$$\\epsilon \\in N(0, s),$$\n",
    "$$s \\in \\{0.1, 1, 10, 25\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load packages\n",
    "using MendelIHT\n",
    "using SnpArrays\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using BenchmarkTools\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "\n",
    "#set random seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "#simulate data\n",
    "n = 5000\n",
    "p = 30000\n",
    "bernoulli_rates = 0.5rand(p) #minor allele frequencies are drawn from uniform (0, 0.5)\n",
    "x = simulate_random_snparray(n, p, bernoulli_rates)\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n",
    "\n",
    "#specify dimension and noise of data\n",
    "k = 10                          # number of true predictors per group\n",
    "S = [0.1, 1.0, 10.0, 25.0]      # noise vector, from very little noise to a lot of noise\n",
    "\n",
    "#construct non-genetic covariates and true model b\n",
    "z           = ones(n, 1)                   # non-genetic covariates, just the intercept\n",
    "true_b      = zeros(p)                     # model vector\n",
    "true_b[1:k] = randn(k)                     # Initialize k non-zero entries in the true model\n",
    "shuffle!(true_b)                           # Shuffle the entries\n",
    "correct_position = findall(x -> x != 0, true_b) # keep track of what the true entries are\n",
    "noise = [rand(Normal(0, s), n) for s in S] # noise vectors from N(0, s) where s ∈ S = {0.01, 0.1, 1, 10}s\n",
    "\n",
    "#simulate phenotypes under different noises by: y = Xb + noise\n",
    "y = [zeros(n) for i in 1:length(S)]\n",
    "for i in 1:length(S)\n",
    "    y[i] = xbm * true_b + noise[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the noisiness of our observations. \n",
    "Each column is the same response vector with varying levels of noise added. The farther right, the noisier the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000×4 Array{Float64,2}:\n",
       "  4.0369     5.57741    -4.36356    -9.6449  \n",
       "  0.191536  -1.04378    -2.07723   -47.0979  \n",
       " -4.38938   -3.34411    -1.42214    24.8877  \n",
       " -0.919674   0.399415  -11.4534    -41.1765  \n",
       "  2.97062    3.56937    -4.50204     4.03327 \n",
       "  2.02283    2.55848    13.8248     -4.50728 \n",
       " -5.78682   -5.10249    10.6672      5.34934 \n",
       " -2.31817   -1.59809     4.346       3.5354  \n",
       " -8.3404    -9.37824   -25.9527    -39.9853  \n",
       " -7.41212   -7.01681     3.36147   -10.8692  \n",
       " -1.88005   -0.969617   -4.15115   -40.838   \n",
       " -1.24192   -1.25763    -5.5715      6.39366 \n",
       " -3.44554   -3.29327     7.4746     16.119   \n",
       "  ⋮                                          \n",
       " -2.89109   -2.74476     2.29241    15.7058  \n",
       " -5.08137   -4.66423    -7.16946   -11.6489  \n",
       "  9.61788    9.90731    28.5531    -23.4735  \n",
       "  8.41764    9.23892     8.4852    -19.3525  \n",
       "  6.27949    5.94512    18.3532      8.64536 \n",
       " -1.43935   -3.03143   -11.0977     -0.475652\n",
       " -4.25876   -3.34605   -26.7249     17.2168  \n",
       " -1.69186   -1.10038   -10.5197    -50.997   \n",
       " -8.41838   -8.01836    -0.631529  -39.2262  \n",
       " -9.46627   -7.39698   -15.4205     15.1138  \n",
       " -0.950841  -1.44118   -20.0629     -1.36569 \n",
       "  0.195906   1.01605     3.26032    11.6767  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y[1] y[2] y[3] y[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Examine reconstruction of IHT given true model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>correct_position</th><th>true_β</th><th>noise_level_1</th><th>noise_level_2</th><th>noise_level_3</th><th>noise_level_4</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>10 rows × 6 columns</p><tr><th>1</th><td>5929</td><td>0.994762</td><td>0.995545</td><td>1.00746</td><td>0.957444</td><td>0.0</td></tr><tr><th>2</th><td>10164</td><td>0.426441</td><td>0.427758</td><td>0.468934</td><td>0.0</td><td>0.0</td></tr><tr><th>3</th><td>11676</td><td>-1.32497</td><td>-1.32341</td><td>-1.33334</td><td>-1.30104</td><td>-1.22798</td></tr><tr><th>4</th><td>14082</td><td>0.38481</td><td>0.386729</td><td>0.387764</td><td>0.0</td><td>0.0</td></tr><tr><th>5</th><td>17954</td><td>-0.628232</td><td>-0.62823</td><td>-0.647939</td><td>-0.83076</td><td>0.0</td></tr><tr><th>6</th><td>18967</td><td>-1.30452</td><td>-1.30594</td><td>-1.29257</td><td>-1.41359</td><td>0.0</td></tr><tr><th>7</th><td>19304</td><td>-1.37284</td><td>-1.37107</td><td>-1.37061</td><td>-1.29244</td><td>-1.54338</td></tr><tr><th>8</th><td>20792</td><td>0.920098</td><td>0.917775</td><td>0.904498</td><td>0.855185</td><td>1.28701</td></tr><tr><th>9</th><td>26349</td><td>-0.322137</td><td>-0.32181</td><td>-0.300086</td><td>0.0</td><td>0.0</td></tr><tr><th>10</th><td>27146</td><td>2.42516</td><td>2.42472</td><td>2.42196</td><td>2.30561</td><td>1.99963</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& correct\\_position & true\\_β & noise\\_level\\_1 & noise\\_level\\_2 & noise\\_level\\_3 & noise\\_level\\_4\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 5929 & 0.994762 & 0.995545 & 1.00746 & 0.957444 & 0.0 \\\\\n",
       "\t2 & 10164 & 0.426441 & 0.427758 & 0.468934 & 0.0 & 0.0 \\\\\n",
       "\t3 & 11676 & -1.32497 & -1.32341 & -1.33334 & -1.30104 & -1.22798 \\\\\n",
       "\t4 & 14082 & 0.38481 & 0.386729 & 0.387764 & 0.0 & 0.0 \\\\\n",
       "\t5 & 17954 & -0.628232 & -0.62823 & -0.647939 & -0.83076 & 0.0 \\\\\n",
       "\t6 & 18967 & -1.30452 & -1.30594 & -1.29257 & -1.41359 & 0.0 \\\\\n",
       "\t7 & 19304 & -1.37284 & -1.37107 & -1.37061 & -1.29244 & -1.54338 \\\\\n",
       "\t8 & 20792 & 0.920098 & 0.917775 & 0.904498 & 0.855185 & 1.28701 \\\\\n",
       "\t9 & 26349 & -0.322137 & -0.32181 & -0.300086 & 0.0 & 0.0 \\\\\n",
       "\t10 & 27146 & 2.42516 & 2.42472 & 2.42196 & 2.30561 & 1.99963 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "10×6 DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ correct_position │ true_β    │ noise_level_1 │ noise_level_2 │\n",
       "│     │ \u001b[90mInt64\u001b[39m            │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m       │ \u001b[90mFloat64\u001b[39m       │\n",
       "├─────┼──────────────────┼───────────┼───────────────┼───────────────┤\n",
       "│ 1   │ 5929             │ 0.994762  │ 0.995545      │ 1.00746       │\n",
       "│ 2   │ 10164            │ 0.426441  │ 0.427758      │ 0.468934      │\n",
       "│ 3   │ 11676            │ -1.32497  │ -1.32341      │ -1.33334      │\n",
       "│ 4   │ 14082            │ 0.38481   │ 0.386729      │ 0.387764      │\n",
       "│ 5   │ 17954            │ -0.628232 │ -0.62823      │ -0.647939     │\n",
       "│ 6   │ 18967            │ -1.30452  │ -1.30594      │ -1.29257      │\n",
       "│ 7   │ 19304            │ -1.37284  │ -1.37107      │ -1.37061      │\n",
       "│ 8   │ 20792            │ 0.920098  │ 0.917775      │ 0.904498      │\n",
       "│ 9   │ 26349            │ -0.322137 │ -0.32181      │ -0.300086     │\n",
       "│ 10  │ 27146            │ 2.42516   │ 2.42472       │ 2.42196       │"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute IHT result for less noisy data\n",
    "estimated_models = [zeros(k) for _ in 1:length(y)]\n",
    "for i in 1:length(y)\n",
    "    result = L0_reg(x, z, y[i], 1, k)\n",
    "    estimated_models[i] .= result.beta[correct_position]\n",
    "end\n",
    "\n",
    "#compare and contrast\n",
    "true_model = true_b[correct_position]\n",
    "compare_model = DataFrame(\n",
    "    correct_position = correct_position, \n",
    "    true_β           = true_model, \n",
    "    noise_level_1    = estimated_models[1],      #N(0, 0.1)\n",
    "    noise_level_2    = estimated_models[2],      #N(0, 1.0)\n",
    "    noise_level_3    = estimated_models[3],      #N(0, 10.0)\n",
    "    noise_level_4    = estimated_models[4])      #N(0, 25.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Interpret reconstruction result\n",
    "\n",
    "IHT finds the correct 10 predictors if we add little noise. With greater noise, we lose predictors with smaller effect size, while losing accuracy. However, found predictors exhibit no shrinkage of effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Using simulated results to examine IHT Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Follow steps 0~2 in example 3 to gain access to multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads() # check that multiple threads are running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Data (as done in Example 4 step 1)\n",
    "\n",
    "### Step 2: Run IHT cross validation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "k\tMSE\n",
      "1\t4.087386566654006\n",
      "2\t3.09896674881206\n",
      "3\t2.235286925818996\n",
      "4\t1.3398166385274393\n",
      "5\t0.8321683827986981\n",
      "6\t0.4285864356097345\n",
      "7\t0.23545804101151324\n",
      "8\t0.14787569877090037\n",
      "9\t0.07123394984266347\n",
      "10\t0.010553689682578828\n",
      "11\t0.013180945864318533\n",
      "12\t0.01321454529484015\n",
      "13\t0.013250436428910795\n",
      "14\t0.013243234830655166\n",
      "15\t0.013259486534082293\n",
      "16\t0.013264444634146878\n",
      "17\t0.013302845545591996\n",
      "18\t0.013308728293452542\n",
      "19\t0.013326603346227688\n",
      "20\t0.013345946393705187\n",
      "\n",
      "The lowest MSE is achieved at k = 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "# specify number of fold and the different model sizes (path)\n",
    "path = collect(1:20)\n",
    "num_folds = 5\n",
    "folds = rand(1:num_folds, size(x, 1))\n",
    "\n",
    "#run cross validation on not-so-noisy data\n",
    "k_est = cv_iht(x, z, y[1], 1, path, folds, num_folds, use_maf = false, debias=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "k\tMSE\n",
      "1\t54.45327294696108\n",
      "2\t53.65021406936241\n",
      "3\t52.773114592990716\n",
      "4\t51.694369916592805\n",
      "5\t51.34685719569556\n",
      "6\t51.19018782283655\n",
      "7\t50.71206240949455\n",
      "8\t50.45521918533876\n",
      "9\t50.62425944709808\n",
      "10\t50.72940821688856\n",
      "11\t50.82354603168742\n",
      "12\t50.99524891915468\n",
      "13\t51.22568897286627\n",
      "14\t51.403841463500804\n",
      "15\t51.451138301758064\n",
      "16\t51.57866320166185\n",
      "17\t51.69300160180016\n",
      "18\t51.77244195146731\n",
      "19\t52.0392801348529\n",
      "20\t52.52346277706467\n",
      "\n",
      "The lowest MSE is achieved at k = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset seed\n",
    "Random.seed!(1111)\n",
    "\n",
    "#run cross validation on pretty noisy data\n",
    "k_est = cv_iht(x, z, y[3], 1, path, folds, num_folds, use_maf = false, debias=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Interpreting results\n",
    "\n",
    "With little noise $\\epsilon \\in N(0, 0.1)$, cross-validation finds the true sparsity parameter: $k_{\\text{estimate}} = k_{\\text{true}} = 10$. For very noisy data $\\epsilon \\in N(0, 10)$, cross validation returns $k_{\\text{estimate}} = 12$, which slightly under-estimates the true sparsity parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated some of the basic features of IHT. The first 3 examples illustrate the basic usage of IHT for general audiences. The last 2 examples illustrate how to make function calls directly and tests its robustness on reproducible simulated data. We selectively omitted several experimental features of IHT. In the near future, we will release a more detailed notebook that includes tutorials on all the experiemental features of IHT on GWAS data as well as motivations for them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
