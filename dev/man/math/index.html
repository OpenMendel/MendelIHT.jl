<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mathematical Details · MendelIHT</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MendelIHT</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Mathematical Details</a><ul class="internal"><li><a class="tocitem" href="#Generalized-linear-models"><span>Generalized linear models</span></a></li><li><a class="tocitem" href="#Loglikelihood,-gradient,-and-expected-information"><span>Loglikelihood, gradient, and expected information</span></a></li><li><a class="tocitem" href="#Iterative-hard-thresholding"><span>Iterative hard thresholding</span></a></li><li><a class="tocitem" href="#Nuisance-parameter-estimation"><span>Nuisance parameter estimation</span></a></li></ul></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../FAQ/">FAQ</a></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Mathematical Details</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mathematical Details</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/OpenMendel/MendelIHT.jl/blob/master/docs/src/man/math.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Details-of-Parameter-Estimation"><a class="docs-heading-anchor" href="#Details-of-Parameter-Estimation">Details of Parameter Estimation</a><a id="Details-of-Parameter-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Details-of-Parameter-Estimation" title="Permalink"></a></h1><p>This note is meant to supplement our <a href="https://doi.org/10.1093/gigascience/giaa044">paper</a>. </p><p>For a review on generalized linear models, the following resources are recommended:</p><ul><li>(3rd edition) Chapter 15.3 of <a href="https://www.amazon.com/Applied-Regression-Analysis-Generalized-Linear/dp/1452205663/ref=sr_1_2?dchild=1&amp;keywords=Applied+Regression+Analysis+and+Generalized+Linear+Models&amp;qid=1609298891&amp;s=books&amp;sr=1-2">Applied regression analysis and generalized linear models</a> by John Fox</li><li>(3rd edition) Chapter 3-5 of <a href="https://www.amazon.com/Introduction-Generalized-Chapman-Statistical-Science/dp/1138741515/ref=sr_1_2?crid=18BN4MONNYYJH&amp;dchild=1&amp;keywords=an+introduction+to+generalized+linear+models&amp;qid=1609298924&amp;s=books&amp;sprefix=an+introduction+to+ge%2Cstripbooks%2C222&amp;sr=1-2">An introduction to generalized linear models</a> by Dobson and Barnett</li></ul><p>For review on projected gradient descent, I recommend</p><ul><li>Chapter 5 of <a href="https://www.amazon.com/MM-Optimization-Algorithms-Kenneth-Lange/dp/1611974399">MM optimization algorithms</a> by Kenneth Lange (2nd edition is almost out, as of 1/2/2021)</li></ul><h2 id="Generalized-linear-models"><a class="docs-heading-anchor" href="#Generalized-linear-models">Generalized linear models</a><a id="Generalized-linear-models-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-linear-models" title="Permalink"></a></h2><p>In <code>MendelIHT.jl</code>, phenotypes <span>$(\bf y)$</span> are modeled as a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear model</a>:</p><p class="math-container">\[\begin{aligned}
    \mu_i = E(y_i) = g({\bf x}_i^t {\boldsymbol \beta})
\end{aligned}\]</p><p>where <span>$\bf x$</span> is sample <span>$i$</span>&#39;s <span>$p$</span>-dimensional vector of <em>covariates</em> (genotypes + other fixed effects), <span>$\boldsymbol \beta$</span> is a <span>$p$</span>-dimensional regression coefficients, <span>$g$</span> is a non-linear <em>inverse-link</em> function, <span>$y_i$</span> is sample <span>$i$</span>&#39;s phenotype value, and <span>$\mu_i$</span> is the <em>average predicted value</em> of <span>$y_i$</span> given <span>$\bf x$</span>. </p><p>The full design matrix <span>${\bf X}_{n \times p}$</span> and phenotypes <span>${\bf y}_{n \times 1}$</span> are observed. The distribution of <span>$\bf y$</span> and the inverse link <span>$g$</span> are chosen before fitting. The regression coefficients <span>$\boldsymbol \beta$</span> are not observed and are estimated by <em>maximum likelihood</em> methods, traditionally via iteratively reweighted least squares (IRLS). For high dimensional problems where <span>$n &lt; p$</span>, we substitute iterative hard thresholding in place of IRLS. </p><p>GLMs offer a natural way to model common non-continuous phenotypes. For instance, logistic regression for binary phenotypes and Poisson regression for integer valued phenotypes are special cases under the GLM framework. When <span>$g(\alpha) = \alpha,$</span> we get standard linear regression used for Gaussian phenotypes. </p><h2 id="Loglikelihood,-gradient,-and-expected-information"><a class="docs-heading-anchor" href="#Loglikelihood,-gradient,-and-expected-information">Loglikelihood, gradient, and expected information</a><a id="Loglikelihood,-gradient,-and-expected-information-1"></a><a class="docs-heading-anchor-permalink" href="#Loglikelihood,-gradient,-and-expected-information" title="Permalink"></a></h2><p>In GLM, the distribution of <span>$\bf y$</span> is from the exponential family with density</p><p class="math-container">\[\begin{aligned}
    f(y \mid \theta, \phi) = \exp \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right].
\end{aligned}\]</p><p>Here <span>$\theta$</span> is called the <em>canonical (location) parameter</em> and under the canonical link, <span>$\theta = g(\bf x^t \bf \beta)$</span>. <span>$\phi$</span> is the <em>dispersion (scale) parameter</em>. The functions <span>$a, b, c$</span> are known functions that vary depending on the distribution of <span>$y$</span>. </p><p>Given <span>$n$</span> independent observations, the loglikelihood is:</p><p class="math-container">\[\begin{aligned}
    L({\bf \theta}, \phi; {\bf y}) &amp;= \sum_{i=1}^n \frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi).
\end{aligned}\]</p><p>To evaluate the loglikelihood, we evaluate sample <span>$i$</span>&#39;s logpdf using the <a href="https://juliastats.org/Distributions.jl/latest/univariate/#Distributions.logpdf-Tuple{Distribution{Univariate,S}%20where%20S%3C:ValueSupport,Real}">logpdf</a> function in <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>.</p><p>The perform maximum likelihood estimation, we compute partial derivatives for <span>$\beta$</span>s. The <span>$j$</span>th score component is (eq 4.18 in Dobson):</p><p class="math-container">\[\begin{aligned}
    \frac{\partial L}{\partial \beta_j} = \sum_{i=1}^n \left[\frac{y_i - \mu_i}{var(y_i)}x_{ij}\left(\frac{\partial \mu_i}{\partial \eta_i}\right)\right].
\end{aligned}\]</p><p>Thus the full <em>gradient</em> is</p><p class="math-container">\[\begin{aligned}
    \nabla L&amp;= {\bf X}^t{\bf W}({\bf y} - \boldsymbol\mu), \quad W_{ii} = \frac{1}{var(y_i)}\left(\frac{\partial \mu_i}{\partial \eta_i}\right),
\end{aligned}\]</p><p>and similarly, the <em>expected information</em> is (eq 4.23 in Dobson):</p><p class="math-container">\[\begin{aligned}
    J = {\bf X^t\tilde{W}X}, \quad \tilde{W}_{ii} = \frac{1}{var(y_i)}\left(\frac{\partial \mu_i}{\partial \eta_i}\right)^2.
\end{aligned}\]</p><p>To evaluate <span>$\nabla L$</span> and <span>$J$</span>, note <span>${\bf y}$</span> and <span>${\bf X}$</span> are known, so we just need to calculate <span>$\boldsymbol\mu, \frac{\partial\mu_i}{\partial\eta_i},$</span> and <span>$var(y_i)$</span>. The first simply uses the inverse link: <span>$\mu_i = g({\bf x}_i^t {\boldsymbol \beta})$</span>. For the second, note <span>$\frac{\partial \mu_i}{\partial\eta_i} = \frac{\partial g({\bf x}_i^t {\boldsymbol \beta})}{\partial{\bf x}_i^t {\boldsymbol \beta}}$</span> is just the derivative of the inverse link function evaluated at the linear predictor <span>$\eta_i = {\bf x}_i^t {\boldsymbol \beta}$</span>. This is already implemented for various link functions as <a href="https://github.com/JuliaStats/GLM.jl/blob/master/src/glmtools.jl#L149">mueta</a> in <a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a>, which we call internally. To compute <span>$var(y_i)$</span>, we note that the exponential family distributions have variance</p><p class="math-container">\[\begin{aligned}
    var(y) &amp;= a(\phi)b&#39;&#39;(\theta) = a(\phi)\frac{\partial^2b(\theta)}{\partial\theta^2} = a(\phi) var(\mu).
\end{aligned}\]</p><p>That is, <span>$var(y_i)$</span> is a product of 2 terms where the first depends solely on <span>$\phi$</span>, and the second solely on <span>$\mu_i = g({\bf x}_i^t {\boldsymbol \beta})$</span>. In our code, we use <a href="https://github.com/JuliaStats/GLM.jl/blob/master/src/glmtools.jl#L315">glmvar</a> implemented in <a href="https://github.com/JuliaStats/GLM.jl">GLM.jl</a> to calculate <span>$var(\mu)$</span>. Because <span>$\phi$</span> is unknown, we assume <span>$a(\phi) = 1$</span> for all models in computing <span>$W_{ii}$</span> and <span>$\tilde{W}_{ii}$</span>, except for the negative binomial model. For negative binomial model, we discuss how to estimate <span>$\phi$</span> and <span>$\boldsymbol\beta$</span> using alternate block descent below.  </p><h2 id="Iterative-hard-thresholding"><a class="docs-heading-anchor" href="#Iterative-hard-thresholding">Iterative hard thresholding</a><a id="Iterative-hard-thresholding-1"></a><a class="docs-heading-anchor-permalink" href="#Iterative-hard-thresholding" title="Permalink"></a></h2><p>In <code>MendelIHT.jl</code>, the loglikelihood is maximized using iterative hard thresholding. This is achieved by repeating the following iteration:</p><p class="math-container">\[\begin{aligned}
    \boldsymbol\beta_{n+1} = \overbrace{P_{S_k}}^{(3)}\big(\boldsymbol\beta_n + \underbrace{s_n}_{(2)} \overbrace{\nabla f(\boldsymbol\beta_n)}^{(1)}\big)
\end{aligned}\]</p><p>where <span>$f$</span> is the loglikelihood to maximize. Step (1) computes the gradient as previously discussed. Step (2) computes the step size <span>$s_k$</span>. Step (3) evaluates the projection operator <span>$P_{S_k}$</span>, which sets all but <span>$k$</span> largest entries in magnitude to <span>$0$</span>. To perform <span>$P_{S_k}$</span>, we first partially sort the <em>dense</em> vector <span>$\beta_n + s_n \nabla f(\beta_n)$</span>, and set the smallest <span>$k+1 ... n$</span> entries in magnitude to <span>$0$</span>. Note the step size <span>$s_n$</span> is derived in our paper to be</p><p class="math-container">\[\begin{aligned}
    s_n = \frac{||\nabla f(\boldsymbol\beta_n)||_2^2}{\nabla f(\boldsymbol\beta_n)^t J(\boldsymbol\beta_n) \nabla f(\boldsymbol\beta_n)}
\end{aligned}\]</p><p>where <span>$J = {\bf X^t\tilde{W}X}$</span> is the expected information matrix (derived in the previous section) <em>which should never be explicitly formed</em>. To evaluate the denominator, observe that </p><p class="math-container">\[\begin{aligned}
    \nabla f(\boldsymbol\beta_n)^t J(\boldsymbol\beta_n) \nabla f(\boldsymbol\beta_n) = \left(\nabla f(\boldsymbol\beta_n)^t{\bf X}^t \sqrt(\tilde{W})\right)\left(\sqrt(\tilde{W}){\bf X}\nabla f(\boldsymbol\beta_n)\right).
\end{aligned}\]</p><p>Thus one computes <span>${\bf v} = \sqrt(\tilde{W}){\bf X}\nabla f(\boldsymbol\beta_n)$</span> and calculate its inner product with itself. </p><h2 id="Nuisance-parameter-estimation"><a class="docs-heading-anchor" href="#Nuisance-parameter-estimation">Nuisance parameter estimation</a><a id="Nuisance-parameter-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Nuisance-parameter-estimation" title="Permalink"></a></h2><p>Currently <code>MendelIHT.jl</code> only estimates nuisance parameter for the Negative Binomial model. Estimation of <span>$\phi$</span> and <span>$\boldsymbol \beta$</span> can be achieved with alternating block updates. That is, we run 1 IHT iteration to estimate <span>$\boldsymbol \beta_n$</span>, followed by 1 iteration of Newton or MM update to estimate <span>$\phi_n$</span>. Below we derive the Newton and MM updates. </p><p>Note 1: This feature is provided by our <a href="https://qcb.ucla.edu/big-summer/big2019-2/">2019 Bruins in Genomics</a> summer student <a href="https://github.com/viviangarcia">Vivian Garcia</a> and <a href="https://github.com/fadusei">Francis Adusei</a>. </p><p>Note 2: for Gaussian response, one can use the sample variance formula to estimate <span>$\phi$</span> from the estimated mean <span>$\hat{\mu}$</span>. </p><h3 id="Parametrization-for-Negative-Binomial-model"><a class="docs-heading-anchor" href="#Parametrization-for-Negative-Binomial-model">Parametrization for Negative Binomial model</a><a id="Parametrization-for-Negative-Binomial-model-1"></a><a class="docs-heading-anchor-permalink" href="#Parametrization-for-Negative-Binomial-model" title="Permalink"></a></h3><p>The negative binomial distribution has density</p><p class="math-container">\[\begin{aligned}
	P(Y = y) = \binom{y+r-1}{y}p^r(1-p)^y
\end{aligned}\]</p><p>where <span>$y$</span> is the number of failures before the <span>$r$</span>th success and <span>$p$</span> is the probability of success in each individual trial. Adhering to these definitions, the mean and variance according to <a href="https://reference.wolfram.com/language/ref/NegativeBinomialDistribution.html">WOLFRAM</a> is </p><p class="math-container">\[\begin{aligned}
	\mu_i = \frac{r(1-p_i)}{r}, \quad
	Var(y_i) = \frac{r(1-p_i)}{p_i^2}.
\end{aligned}\]</p><p>Note these formula are different than the default on <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">wikipedia</a> because in wiki <span>$y$</span> is the number of <em>success</em> and <span>$r$</span> is the number of <em>failure</em>.  Therefore, solving for <span>$p_i$</span>, we have </p><p class="math-container">\[\begin{aligned}
	p_i = \frac{r}{\mu_i + r} = \frac{r}{e^{\mathbf{x}_i^T\beta} + r} \in (0, 1).
\end{aligned}\]</p><p>And indeed this this is how we <a href="https://github.com/OpenMendel/MendelIHT.jl/blob/master/src/utilities.jl#L41">parametrize the negative binomial model</a>. <strong>Importantly, we can interpret <span>$p_i$</span> as a probability</strong>, since <span>$\mathbf{x}_i^T\beta$</span> can take on any number between <span>$-\infty$</span> and <span>$+\infty$</span> (since <span>$\beta$</span> and <span>$\mathbf{x}_i$</span> can have positive and negative entries), so <span>$exp(\mathbf{x}_i^T\beta)\in(0, \infty)$</span>.</p><p>We can also try to express <span>$Var(y_i)$</span> in terms of <span>$\mu_i$</span> and <span>$r$</span> by doing some algebra:</p><p class="math-container">\[\begin{aligned}
	Var(y_i)
	&amp;= \frac{r(1-p_i)}{p_i^2} = \frac{r\left( 1 - \frac{r}{\mu_i + r} \right)}{\frac{r^2}{(\mu_i + r)^2}} = \frac{1}{r}\left(1 - \frac{r}{\mu_i + r}\right)(\mu_i + r)^2 \\
	&amp;= \frac{1}{r} \left[ (\mu_i + r)^2 - r(\mu_r + r) \right] = \frac{1}{r}(\mu_i + r)\mu_i\\
	&amp;= \mu_i \left( \frac{\mu_i}{r} + 1 \right)
\end{aligned}\]</p><p>You can verify <a href="https://github.com/JuliaStats/GLM.jl/blob/ef246bb8fdbfa3f3058435035d0b0cf42abdd06e/src/glmtools.jl#L320">in GLM.jl</a> that this is indeed how they compute the variance of a negative binomial distribution. </p><h3 id="Estimating-nuisance-parameter-using-MM-algorithms"><a class="docs-heading-anchor" href="#Estimating-nuisance-parameter-using-MM-algorithms">Estimating nuisance parameter using MM algorithms</a><a id="Estimating-nuisance-parameter-using-MM-algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Estimating-nuisance-parameter-using-MM-algorithms" title="Permalink"></a></h3><p>The MM algorithm is very stable, but converges much slower than Newton&#39;s alogorithm below. Thus use MM only if Newton&#39;s method fails.</p><p>The loglikelihood for <span>$n$</span> independent samples under a Negative Binomial model is </p><p class="math-container">\[\begin{aligned}
	L(p_1, ..., p_m, r)
	&amp;= \sum_{i=1}^m \ln \binom{y_i+r-1}{y_i} + r\ln(p_i) + y_i\ln(1-p_i)\\
	&amp;= \sum_{i=1}^m \left[ \sum_{j=0}^{y_i - 1} \ln(r+j) + r\ln(p_i) - \ln(y_i!) + y_i\ln(1-p_i) \right]\\
	&amp;\geq \sum_{i=1}^m\left[ \sum_{j=0}^{y_i-1}\frac{r_n}{r_n+j}\ln(r) + c_n + r\ln(p_i) - \ln(y_i!) + y_i \ln(1-p_i) \right]\\
    &amp;\equiv M(p_1, ..., p_m, r)
\end{aligned}\]</p><p>The last inequality can be seen by applying Jensen&#39;s inequality:</p><p class="math-container">\[\begin{aligned}
	f\left[ \sum_{i}u_i(\boldsymbol\theta)\right] \leq \sum_{i} \frac{u_i(\boldsymbol\theta_n)}{\sum_j u_j(\boldsymbol\theta_n)}f \left[ \frac{\sum_j u_j(\boldsymbol\theta_n)}{u_i(\boldsymbol\theta_n)} u_i(\boldsymbol\theta)\right]
\end{aligned}\]</p><p>to the function <span>$f(u) = - \ln(u).$</span> Maximizing <span>$M$</span> over <span>$r$</span> (i.e. differentiating with respect to <span>$r$</span> and setting equal to zero, then solving for <span>$r$</span>), we have</p><p class="math-container">\[\begin{aligned}
    \frac{d}{dr} M
	&amp;= \sum_{i=1}^{m} \left[ \sum_{j=0}^{y_i-1} \frac{r_n}{r_n + j} \frac{1}{r} + \ln(p_i) \right] \\
	&amp;= \sum_{i=1}^{m}\sum_{j=0}^{y_i-1} \frac{r_n}{r_n + j} \frac{1}{r} + \sum_{i=1}^{m}\ln(p_i)\\ 
	&amp;\equiv 0\\
	\iff r_{n+1} &amp;= \frac{-\sum_{i=1}^{m}\sum_{j=0}^{y_i-1} \frac{r_n}{r_n + j}}{\sum_{i=1}^{m}\ln(p_i) } 
\end{aligned}\]</p><p>Since <span>$L \ge M$</span> (M <em>minorizes</em> L), maximizing <span>$M$</span> will maximize <span>$L$</span>. </p><h3 id="Estimating-Nuisance-parameter-using-Newton&#39;s-method"><a class="docs-heading-anchor" href="#Estimating-Nuisance-parameter-using-Newton&#39;s-method">Estimating Nuisance parameter using Newton&#39;s method</a><a id="Estimating-Nuisance-parameter-using-Newton&#39;s-method-1"></a><a class="docs-heading-anchor-permalink" href="#Estimating-Nuisance-parameter-using-Newton&#39;s-method" title="Permalink"></a></h3><p>Since we are dealing with 1 parameter optimization, Newton&#39;s method is likely a better candidate due to its quadratic rate of convergence. To estimate the nuisance parameter (<span>$r$</span>), we use maximum likelihood estimates. By <span>$p_i = r / (\mu_i + r)$</span> in above, we have</p><p class="math-container">\[\begin{aligned}
	&amp; L(p_1, ..., p_m, r)\\
	=&amp; \sum_{i=1}^m \ln \binom{y_i+r-1}{y_i} + r\ln(p_i) + y_i\ln(1-p_i)\\
	=&amp; \sum_{i=1}^m \left[ \ln\left((y_i+r-1)!\right) - \ln\left(y_i!\right) - \ln\left((r-1)!\right) + r\ln(r) - r\ln(\mu_i+r) + y_i\ln(\mu_i) + y_i\ln(\mu_i + r)\right]\\
	=&amp; \sum_{i=1}^m\left[\ln\left((y_i+r-1)!\right)-\ln(y_i!) - \ln\left((r-1)\right) + r\ln(r) - (r+y_i)\ln(\mu_i + r) + y_i\ln(\mu_i)\right]
\end{aligned}\]</p><p>Recalling the definition of <a href="https://en.wikipedia.org/wiki/Digamma_function">digamma and trigamma functions</a>, the first and second derivative of our last expression with respect to <span>$r$</span> is:</p><p class="math-container">\[\begin{aligned}
	\frac{d}{dr} L(p_1, ..., p_m, r) = &amp; \sum_{i=1}^m \left[ \operatorname{digamma}(y_i+r) - \operatorname{digamma}(r) + 1 + \ln(r) - \frac{r+y_i}{\mu_i+r} - \ln(\mu_i + r) \right]\\
	\frac{d^2}{dr^2} L(p_1, ..., p_m, r) =&amp;\sum_{i=1}^m \left[ \operatorname{trigamma}(y_i+r) - \operatorname{trigamma}(r) + \frac{1}{r} - \frac{2}{\mu_i + r} + \frac{r+y_i}{(\mu_i + r)^2} \right]
\end{aligned}\]</p><p>So the iteration to use is:</p><p class="math-container">\[\begin{aligned}
	r_{n+1} = r_n - \frac{\frac{d}{dr}L(p_1,...,p_m,r)}{\frac{d^2}{dr^2}L(p_1,...,p_m,r)}.
\end{aligned}\]</p><p>For stability, we set the denominator equal to <span>$1$</span> if it is less than 0. That is, we use gradient descent if the current iteration has non-positive definite Hessian matrices. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../contributing/">Contributing »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.16 on <span class="colophon-date" title="Tuesday 26 April 2022 02:51">Tuesday 26 April 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
